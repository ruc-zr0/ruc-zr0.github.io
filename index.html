<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description"
    content="ZR-0 is pre-trained on a diverse mixture of cross-embodiment manipulation data and general vision–language corpora (e.g., VQA and image captioning). The pre-training dataset comprises over 400k trajectories collected from a wide range of robot embodiments, including Franka, xArm, GR-1, ALOHA, ARX5, UR5, and others, covering diverse environments and execution styles.">
  <meta name="keywords" content="Robotics, VLA, Embodied AI, Chain-of-Thought, ZR-0">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ZR-0: Pre-Training Generalist Vision–Language–Action Model with Embodied Chain-of-Thought Reasoning</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>

  <nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
      <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
        <span aria-hidden="true"></span>
      </a>
    </div>
    <div class="navbar-menu">
      <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
        <a class="navbar-item" href="https://keunhong.com">
          <span class="icon">
            <i class="fas fa-home"></i>
          </span>
        </a>

        <div class="navbar-item has-dropdown is-hoverable">
          <a class="navbar-link">
            More Research
          </a>
          <div class="navbar-dropdown">
            <a class="navbar-item" href="https://hypernerf.github.io">
              HyperNeRF
            </a>
            <a class="navbar-item" href="https://nerfies.github.io">
              Nerfies
            </a>
            <a class="navbar-item" href="https://latentfusion.github.io">
              LatentFusion
            </a>
            <a class="navbar-item" href="https://photoshape.github.io">
              PhotoShape
            </a>
          </div>
        </div>
      </div>

    </div>
  </nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ZR-0: Pre-Training Generalist Vision–Language–Action Model with
              Embodied Chain-of-Thought Reasoning</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://keunhong.com">Author 1</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://utkarshsinha.com">Author 2</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://jonbarron.info">Author 3</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="http://sofienbouaziz.com">Author 4</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://www.danbgoldman.com">Author 5</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="https://homes.cs.washington.edu/~seitz/">Author 6</a><sup>1,2</sup>,
              </span>
              <span class="author-block">
                <a href="http://www.ricardomartinbrualla.com">Author 7</a><sup>2</sup>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Institution 1,</span>
              <span class="author-block"><sup>2</sup>Institution 2</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2011.12948" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Video Link. -->
                <span class="link-block">
                  <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-youtube"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/google/nerfies" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/google/nerfies/releases/tag/0.1"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">

          <div class="content has-text-justified">
            <img src="./static/images/model_architecture.png" class="interpolation-image"
              alt="The ZR-0 architecture." />
          </div>
          <h2 class="subtitle has-text-centered">
            <span class="dnerf">ZR-0</span> is a generalist vision–language–action model that
            leverages embodied chain-of-thought reasoning to solve complex multi-step robotic tasks.
          </h2>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">What is ZR-0?</h2>
          <div class="content has-text-justified">
            <p>
              <span class="dnerf"><strong>ZR-0</strong></span> is a generalist VLA model <strong>pre-trained on over
                400k robotic manipulation trajectories across diverse embodiments</strong>
              (e.g., Franka, xArm, GR-1, ALOHA, ARX5, UR5) as well as general vision-language corpora (e.g., VQA, image
              captioning).
            </p>
            <p>
              Pre-training required over 13,000 H100 GPU hours.
            </p>
            <p>
              A unique aspect of ZR-0 is the explicit annotation of every frame with <span
                class="dnerf"><strong>Embodied Chain-of-Thought (ECoT) reasoning traces</strong></span> , supporting
              grounded, multi-step decision making for robotics tasks.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3">ECOT</h3>
        <div class="content has-text-justified">
          <img src="./static/images/embodied_cot.png" class="interpolation-image" alt="The ZR-0 architecture." />
        </div>
      </div>
    </div>

    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h3 class="title is-3"></h3>
          <div class="content has-text-justified">
            <p>
              <strong>Joint Reasoning and Control Learning: </strong>ZR-0 is jointly trained with two objectives: a
              next-token prediction loss for ECoT reasoning, and a flow-matching loss for continuous action chunk
              generation.
              This unified approach enables the model to integrate vision-language reasoning, task planning, spatial and
              environment comprehension, and robotic action at every step.
              We believe that large-scale co-training in this manner effectively bridges the structural gap between
              vision-language models (VLM) and vision-language-action (VLA) systems.
            </p>
            <p>
              <strong>Generalization & Adaptability: </strong>Thanks to ECoT supervision, ZR-0 demonstrates strong
              zero-shot performance and can rapidly adapt to new robot embodiments, environments, or tasks with
              lightweight finetuning.
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Key Features</h2>
          <div class="content has-text-justified">
            <ul>
              <li><strong>2.6 Billion Parameters:</strong> System 2 + System 1 hybrid design.
                <ul>
                  <li><strong>System 2:</strong> Vision-Language backbone (Qwen3-VL-2B)</li>
                  <li><strong>System 1:</strong> Flow-matching-based action policy for chunked, continuous action generation</li>
                </ul>
              </li>
              <li><strong>ECoT Supervision:</strong> Only during training, improving reasoning and embodied representations; not required at inference time.</li>
              <li><strong>Fast, Efficient Inference:</strong> Few-step diffusion decoding—no reliance on slow autoregressive generation at deployment.</li>
              <li><strong>Out-of-the-Box Strong Zero-Shot Performance:</strong> High competence even prior to fine-tuning.</li>
              <li><strong>Highly Customizable:</strong> Fine-tune on arbitrary robots, tasks, or environments.</li>
            </ul>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
    </div>
  </section>



  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
        <a class="icon-link" href="./static/videos/nerfies_paper.pdf">
          <i class="fas fa-file-pdf"></i>
        </a>
        <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
          <i class="fab fa-github"></i>
        </a>
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              This means you are free to borrow the <a href="https://github.com/nerfies/nerfies.github.io">source
                code</a> of this website,
              we just ask that you link back to this page in the footer.
              Please remember to remove the analytics code included in the header of the website which
              you do not want on your website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>